{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ded730-ab0e-4c31-b506-8702ee0e263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aymenbj/AERL39_2/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "\n",
    "# Function to append logs\n",
    "def append_log(content):\n",
    "    \"\"\"Appends a string to the log file inside the model directory.\"\"\"\n",
    "    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "    with open(log_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(content + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f061b2b-b5db-456c-b1ee-beef33e61a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ismModifDoss=\"DETERMINISTIC_Layer_original_acc\"\n",
    "ismModif=\"100e_s40\"\n",
    "Modif_desc=\"\"\n",
    "ismModifDataset=\"DETERMINISTIC\"\n",
    "ismModifN=\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define log file path with _\"+ismModif\"\n",
    "\n",
    "model_dir = \"models_AWID_\" + ismModifDoss\n",
    "log_file_path = os.path.join(model_dir, \"training_log_\" + ismModif + \".txt\")\n",
    "append_log(Modif_desc+'\\n')\n",
    "h5_model_path = os.path.join(model_dir, \"defender_agent_model_Full_\" + ismModif + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bff7d78-e0e6-4fdf-8417-712fc1eaa927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score,recall_score, precision_score\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Import definitions from your training file.\n",
    "# It is assumed that this file defines the following:\n",
    "#   - ismModifDoss, ismModif, ismModifDataset (used for file naming)\n",
    "#   - RLenv: the environment class for loading/formatting data\n",
    "#   - huber_loss: your custom loss function\n",
    "# (Other classes such as DefenderAgent, etc., are defined but for AERL fine tuning\n",
    "# we work directly with the defender’s Keras model.)\n",
    "\n",
    "from gan_model import * \n",
    "\n",
    "import tensorflow.keras.backend as K  # ✅ Import K globally\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f8dede-3a2e-4be1-a4ab-9eb3b991fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"splitted/train.csv\"\n",
    "test_path = \"splitted/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b0be90-753c-4fdc-8c61-4d30b1a792cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress INFO, WARNING, and DEBUG messages\n",
    "import tensorflow as tf\n",
    "#tf.get_logger().setLevel('ERROR')  # Only show errors from TensorFlow's logger\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class data_cls:\n",
    "    def __init__(self,train_test):\n",
    "        \n",
    "       \n",
    "        self.index = 0\n",
    "        if(train_test==\"train\"):\n",
    "          self.train_path = train_path\n",
    "        else:\n",
    "          self.train_path = test_path\n",
    "       \n",
    "   \n",
    "        self.df = pd.read_csv(self.train_path,sep=',')\n",
    "    \n",
    "\n",
    "         # One hot encoding for labels\n",
    "        self.df = pd.concat([self.df.drop('labels', axis=1),\n",
    "                pd.get_dummies(self.df['labels'])], axis=1)\n",
    "        \n",
    "\n",
    "        self.attack_types = ['normal', 'flooding', 'injection', 'impersonation']\n",
    "        self.all_attack_types = ['normal', 'flooding', 'injection', 'impersonation']\n",
    "        self.attack_names = ['normal', 'flooding', 'injection', 'impersonation']\n",
    "        \n",
    "\n",
    "        self.attack_map =   { 'normal': 'normal','flooding': 'flooding', 'injection':'injection', 'impersonation':'impersonation'}\n",
    "        \n",
    "        self.all_attack_names = list(self.attack_map.keys())\n",
    "\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "\n",
    "    def get_shape(self):\n",
    "              \n",
    "        self.data_shape = self.df.shape\n",
    "        # stata + labels\n",
    "        return self.data_shape\n",
    "    \n",
    "    ''' Get n-rows from loaded data \n",
    "        The dataset must be loaded in RAM\n",
    "    '''\n",
    "    def get_batch(self,batch_size=100):\n",
    "                \n",
    "        # Read the df rows\n",
    "        indexes = list(range(self.index,self.index+batch_size))    \n",
    "        if max(indexes)>self.data_shape[0]-1:\n",
    "            dif = max(indexes)-self.data_shape[0]\n",
    "            indexes[len(indexes)-dif-1:len(indexes)] = list(range(dif+1))\n",
    "            self.index=batch_size-dif\n",
    "            batch = self.df.iloc[indexes]\n",
    "        else: \n",
    "            batch = self.df.iloc[indexes]\n",
    "            self.index += batch_size    \n",
    "            \n",
    "        labels = batch[self.attack_names]\n",
    "        \n",
    "        batch = batch.drop(self.all_attack_names,axis=1)\n",
    "            \n",
    "        return batch,labels\n",
    "    \n",
    "    def get_full(self):\n",
    "              \n",
    "        labels = self.df[self.attack_names]\n",
    "        \n",
    "        batch = self.df.drop(self.all_attack_names,axis=1)\n",
    "        \n",
    "\n",
    "        return batch,labels \n",
    "  \n",
    "  # Huber loss function        \n",
    "def huber_loss(y_true, y_pred, clip_value=1):\n",
    "    # Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n",
    "    # https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
    "    # for details.\n",
    "    assert clip_value > 0.\n",
    "\n",
    "    x = y_true - y_pred\n",
    "    if np.isinf(clip_value):\n",
    "        # Spacial case for infinity since Tensorflow does have problems\n",
    "        # if we compare `K.abs(x) < np.inf`.\n",
    "        return .5 * K.square(x)\n",
    "\n",
    "    condition = K.abs(x) < clip_value\n",
    "    squared_loss = .5 * K.square(x)\n",
    "    linear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n",
    "    if K.backend() == 'tensorflow':\n",
    "        \n",
    "        if hasattr(tf, 'select'):\n",
    "            return tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n",
    "        else:\n",
    "            return tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n",
    "    elif K.backend() == 'theano':\n",
    "        from theano import tensor as T\n",
    "        return T.switch(condition, squared_loss, linear_loss)\n",
    "    else:\n",
    "        raise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n",
    "\n",
    "# Needed for keras huber_loss locate\n",
    "import tensorflow.keras.losses\n",
    "tensorflow.keras.losses.huber_loss = huber_loss\n",
    "\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "class QNetwork():\n",
    "    \"\"\"\n",
    "    Q-Network Estimator\n",
    "    Represents the global model for the table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,obs_size,num_actions,hidden_size = 100,\n",
    "                 hidden_layers = 1,learning_rate=0.00025):\n",
    "        \"\"\"\n",
    "        Initialize the network with the provided shape\n",
    "        \"\"\"\n",
    "        self.obs_size = obs_size\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Network arquitecture\n",
    "        self.model = Sequential()\n",
    "        # Add imput layer\n",
    "        self.model.add(Dense(hidden_size, input_shape=(obs_size,),\n",
    "                             activation='relu'))\n",
    "        # Add hidden layers\n",
    "        for layers in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        # Add output layer    \n",
    "        self.model.add(Dense(num_actions))\n",
    "        \n",
    "        # optimizer = optimizers.SGD(learning_rate)\n",
    "        optimizer = optimizers.Adam(learning_rate)\n",
    "        # optimizer = optimizers.Adam(0.00025)\n",
    "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
    "        \n",
    "        # Compilation of the model with optimizer and loss\n",
    "        self.model.compile(loss=huber_loss,optimizer=optimizer)\n",
    "\n",
    "    def predict(self,state,batch_size=1):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(state,batch_size=batch_size)\n",
    "\n",
    "    def update(self, states, q):\n",
    "        \"\"\"\n",
    "        Updates the estimator with the targets.\n",
    "\n",
    "        Args:\n",
    "          states: Target states\n",
    "          q: Estimated values\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        loss = self.model.train_on_batch(states, q)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def copy_model(model):\n",
    "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
    "        cloned_model = clone_model(model)\n",
    "        cloned_model.set_weights(model.get_weights())\n",
    "        return cloned_model\n",
    "\n",
    "#Policy interface\n",
    "class Policy:\n",
    "    def __init__(self, num_actions, estimator):\n",
    "        self.num_actions = num_actions\n",
    "        self.estimator = estimator\n",
    "    \n",
    "class Epsilon_greedy(Policy):\n",
    "    def __init__(self,estimator ,num_actions ,epsilon,min_epsilon,decay_rate, epoch_length):\n",
    "        Policy.__init__(self, num_actions, estimator)\n",
    "        self.name = \"Epsilon Greedy\"\n",
    "        \n",
    "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
    "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
    "            sys.exit(0)\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.actions = list(range(num_actions))\n",
    "        self.step_counter = 0\n",
    "        self.epoch_length = epoch_length\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        #if epsilon is up 0.1, it will be decayed over time\n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon_decay = True\n",
    "        else:\n",
    "            self.epsilon_decay = False\n",
    "    \n",
    "    def get_actions(self,states):\n",
    "        # get next action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            actions = np.random.randint(0, self.num_actions,states.shape[0])\n",
    "        else:\n",
    "            self.Q = self.estimator.predict(states,states.shape[0])\n",
    "            actions = []\n",
    "            for row in range(self.Q.shape[0]):\n",
    "                best_actions = np.argwhere(self.Q[row] == np.amax(self.Q[row]))\n",
    "                actions.append(best_actions[np.random.choice(len(best_actions))].item())\n",
    "            \n",
    "        self.step_counter += 1 \n",
    "        # decay epsilon after each epoch\n",
    "        if self.epsilon_decay:\n",
    "            if self.step_counter % self.epoch_length == 0:\n",
    "                self.epsilon = max(self.min_epsilon, self.epsilon * self.decay_rate**self.step_counter)\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size,self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "\n",
    "        self.num_observed += 1\n",
    "\n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "\n",
    "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
    "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
    "\n",
    "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)\n",
    "\n",
    "\n",
    "'''\n",
    "Reinforcement learning Agent definition\n",
    "'''\n",
    "\n",
    "class Agent(object):  \n",
    "        \n",
    "    def __init__(self, actions,obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "        self.obs_size = obs_size\n",
    "        \n",
    "        self.epsilon = kwargs.get('epsilon', 1)\n",
    "        self.min_epsilon = kwargs.get('min_epsilon', .1)\n",
    "        self.gamma = kwargs.get('gamma', .001)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        self.decay_rate = kwargs.get('decay_rate',0.99)\n",
    "        self.ExpRep = kwargs.get('ExpRep',True)\n",
    "        if self.ExpRep:\n",
    "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
    "        \n",
    "        self.ddqn_time = 100\n",
    "        self.ddqn_update = self.ddqn_time\n",
    "\n",
    "        \n",
    "        self.model_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers',1),\n",
    "                                      kwargs.get('learning_rate',.00025))\n",
    "        self.target_model_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers',1),\n",
    "                                      kwargs.get('learning_rate',.00025))\n",
    "        self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
    "        \n",
    "        if policy == \"EpsilonGreedy\":\n",
    "            self.policy = Epsilon_greedy(self.model_network,len(actions),\n",
    "                                         self.epsilon,self.min_epsilon,\n",
    "                                         self.decay_rate,self.epoch_length)\n",
    "        \n",
    "        \n",
    "    def learn(self, states, actions,next_states, rewards, done):\n",
    "        if self.ExpRep:\n",
    "            self.memory.observe(states, actions, rewards, done)\n",
    "        else:\n",
    "            self.states = states\n",
    "            self.actions = actions\n",
    "            self.next_states = next_states\n",
    "            self.rewards = rewards\n",
    "            self.done = done        \n",
    "    def update_model(self):\n",
    "        if self.ExpRep:\n",
    "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "        else:\n",
    "            states = self.states\n",
    "            rewards = self.rewards\n",
    "            next_states = self.next_states\n",
    "            actions = self.actions\n",
    "            done = self.done\n",
    "        \n",
    "        next_actions = []\n",
    "        # Compute Q targets\n",
    "#        Q_prime = self.model_network.predict(next_states,self.minibatch_size)\n",
    "        Q_prime = self.target_model_network.predict(next_states,self.minibatch_size)\n",
    "        # TODO: fix performance in this loop\n",
    "        for row in range(Q_prime.shape[0]):\n",
    "            best_next_actions = np.argwhere(Q_prime[row] == np.amax(Q_prime[row]))\n",
    "            next_actions.append(best_next_actions[np.random.choice(len(best_next_actions))].item())\n",
    "        sx = np.arange(len(next_actions))\n",
    "        # Compute Q(s,a)\n",
    "        Q = self.model_network.predict(states,self.minibatch_size)\n",
    "        # Q-learning update\n",
    "        # target = reward + gamma * max_a'{Q(next_state,next_action))}\n",
    "        targets = rewards.reshape(Q[sx,actions].shape) + \\\n",
    "                  self.gamma * Q[sx,next_actions] * \\\n",
    "                  (1-done.reshape(Q[sx,actions].shape))   \n",
    "        Q[sx,actions] = targets  \n",
    "        \n",
    "        loss = self.model_network.model.train_on_batch(states,Q)#inputs,targets        \n",
    "        \n",
    "        # timer to ddqn update\n",
    "        self.ddqn_update -= 1\n",
    "        if self.ddqn_update == 0:\n",
    "            self.ddqn_update = self.ddqn_time\n",
    "#            self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
    "            self.target_model_network.model.set_weights(self.model_network.model.get_weights()) \n",
    "        \n",
    "        return loss    \n",
    "\n",
    "    def act(self, state,policy):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class DefenderAgent(Agent):      \n",
    "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
    "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", ** kwargs)\n",
    "        \n",
    "    def act(self,states):\n",
    "        # Get actions under the policy\n",
    "        actions = self.policy.get_actions(states)\n",
    "        return actions\n",
    "    \n",
    "class AttackAgent(Agent):      \n",
    "    def __init__(self, actions, obs_size, policy=\"EpsilonGreedy\", **kwargs):\n",
    "        super().__init__(actions,obs_size, policy=\"EpsilonGreedy\", ** kwargs)\n",
    "        \n",
    "    def act(self,states):\n",
    "        # Get actions under the policy\n",
    "        actions = self.policy.get_actions(states)\n",
    "        return actions\n",
    "    \n",
    "'''\n",
    "Reinforcement learning Enviroment Definition\n",
    "'''\n",
    "class RLenv(data_cls):\n",
    "    def __init__(self,train_test,**kwargs):\n",
    "        data_cls.__init__(self,train_test)\n",
    "        self.data_shape = data_cls.get_shape(self)\n",
    "        self.batch_size = kwargs.get('batch_size',1) # experience replay -> batch = 1\n",
    "        self.iterations_episode = kwargs.get('iterations_episode',10)\n",
    "        if self.batch_size=='full':\n",
    "            self.batch_size = int(self.data_shape[0]/iterations_episode)\n",
    "\n",
    "    '''\n",
    "    _update_state: function to update the current state\n",
    "    Returns:\n",
    "        None\n",
    "    Modifies the self parameters involved in the state:\n",
    "        self.state and self.labels\n",
    "    Also modifies the true labels to get learning knowledge\n",
    "    '''\n",
    "    def _update_state(self):        \n",
    "        self.states,self.labels = data_cls.get_batch(self)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.true_labels += np.sum(self.labels).values\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        + Observation of the enviroment\n",
    "    '''\n",
    "    def reset(self):\n",
    "        # Statistics\n",
    "        self.def_true_labels = np.zeros(len(self.attack_types),dtype=int)\n",
    "        self.def_estimated_labels = np.zeros(len(self.attack_types),dtype=int)\n",
    "        self.att_true_labels = np.zeros(len(self.attack_names),dtype=int)\n",
    "        \n",
    "        self.state_numb = 0\n",
    "        \n",
    "        self.states,self.labels = data_cls.get_batch(self,self.batch_size)\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        self.steps_in_episode = 0\n",
    "        return self.states.values \n",
    "   \n",
    "    '''\n",
    "    Returns:\n",
    "        State: Next state for the game\n",
    "        Reward: Actual reward\n",
    "        done: If the game ends (no end in this case)\n",
    "    \n",
    "    In the adversarial enviroment, it's only needed to return the actual reward\n",
    "    '''    \n",
    "    def act(self,defender_actions,attack_actions):\n",
    "        # Clear previous rewards        \n",
    "        self.att_reward = np.zeros(len(attack_actions))       \n",
    "        self.def_reward = np.zeros(len(defender_actions))\n",
    "        \n",
    "        \n",
    "        attack = [self.attack_types.index(self.attack_map[self.attack_names[att]]) for att in attack_actions]\n",
    "        \n",
    "        self.def_reward = (np.asarray(defender_actions)==np.asarray(attack))*1\n",
    "        self.att_reward = (np.asarray(defender_actions)!=np.asarray(attack))*1\n",
    "\n",
    "         \n",
    "       \n",
    "        self.def_estimated_labels += np.bincount(defender_actions,minlength=len(self.attack_types))\n",
    "        # TODO\n",
    "        # list comprehension\n",
    "        \n",
    "        for act in attack_actions:\n",
    "            self.def_true_labels[self.attack_types.index(self.attack_map[self.attack_names[act]])] += 1\n",
    "        \n",
    "\n",
    "        # Get new state and new true values \n",
    "        attack_actions = attacker_agent.act(self.states)\n",
    "        self.states = env.get_states(attack_actions)\n",
    "        \n",
    "        # Done allways false in this continuous task       \n",
    "        self.done = np.zeros(len(attack_actions),dtype=bool)\n",
    "            \n",
    "        return self.states, self.def_reward,self.att_reward, attack_actions, self.done\n",
    "    \n",
    "    '''\n",
    "    Provide the actual states for the selected attacker actions\n",
    "    Parameters:\n",
    "        self:\n",
    "        attacker_actions: optimum attacks selected by the attacker\n",
    "            it can be one of attack_names list and select random of this\n",
    "    Returns:\n",
    "        State: Actual state for the selected attacks\n",
    "    '''\n",
    "    def get_states(self,attacker_actions):\n",
    "        first = True\n",
    "        for attack in attacker_actions:\n",
    "            if first:\n",
    "                minibatch = (self.df[self.df[self.attack_names[attack]]==1].sample(1))\n",
    "                first = False\n",
    "            else:\n",
    "                minibatch=minibatch.append(self.df[self.df[self.attack_names[attack]]==1].sample(1))\n",
    "        \n",
    "        self.labels = minibatch[self.attack_names]\n",
    "        minibatch.drop(self.all_attack_names,axis=1,inplace=True)\n",
    "        self.states = minibatch\n",
    "        \n",
    "        return self.states\n",
    "\n",
    "test_loss_chain = [] #MOD-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b5e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal', 'flooding', 'injection', 'impersonation']\n",
      "46\n",
      "(1795575, 50)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts before adding synthetic data:\n",
      "normal           1633190\n",
      "flooding           48484\n",
      "injection          65379\n",
      "impersonation      48522\n",
      "dtype: int64\n",
      "Label counts after adding synthetic data:\n",
      "normal           1633190\n",
      "flooding           48484\n",
      "injection          65379\n",
      "impersonation      48522\n",
      "dtype: int64\n",
      "Number of labels added (per attack class):\n",
      "normal           0\n",
      "flooding         0\n",
      "injection        0\n",
      "impersonation    0\n",
      "dtype: int64\n",
      "\n",
      "Total number of samples for each label (before and after):\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Total epoch: 100 | Iterations in epoch: 100 | Minibatch from mem size: 100 | Total Samples: 10000|\n",
      "-------------------------------------------------------------------------------\n",
      "Dataset shape: (1795575, 50)\n",
      "-------------------------------------------------------------------------------\n",
      "Attacker parameters: Num_actions=4 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=1|\n",
      "-------------------------------------------------------------------------------\n",
      "Defense parameters: Num_actions=4 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=2|\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------- Hyperparameter Details ---------------------- #\n",
    "# \n",
    "# 1. **GAN Parameters (Generative Adversarial Network)**\n",
    "#    - `batch_size`: Number of samples per batch during training.\n",
    "#    - `learning_rate (0.0002)`: Controls how much the model updates per step.\n",
    "#    - `latent_dim (100)`: The size of the random noise vector for generating data.\n",
    "#    - `input_dim (states.shape[1])`: The number of features in the input data.\n",
    "#    - `hidden_dim (128)`: Number of neurons in hidden layers of the GAN.\n",
    "#    - `num_classes (len(env.attack_types))`: Number of different attack types to classify.\n",
    "#\n",
    "# 2. **Training Parameters**\n",
    "#    - `num_episodes (50)`: Total number of training episodes.\n",
    "#    - `iterations_episode (100)`: Number of steps per episode.\n",
    "#    - `minibatch_size (100)`: Number of samples to use per gradient update.\n",
    "#    - `ExpRep (True)`: Whether to use Experience Replay for training.\n",
    "#\n",
    "# 3. **Defender Agent Hyperparameters**\n",
    "#    - `epsilon (1)`: Initial exploration rate for the defender.\n",
    "#    - `min_epsilon (0.01)`: Minimum exploration rate.\n",
    "#    - `gamma (0.001)`: Discount factor for future rewards.\n",
    "#    - `decay_rate (0.99)`: Rate at which epsilon decays.\n",
    "#    - `hidden_size (100)`: Number of neurons per hidden layer.\n",
    "#    - `hidden_layers (5)`: Number of hidden layers in the model.\n",
    "#    - `learning_rate (0.2)`: Learning rate for the defender.\n",
    "#    - `mem_size (1000)`: Size of the replay memory.\n",
    "#\n",
    "# 4. **Attacker Agent Hyperparameters**\n",
    "#    - `epsilon (1)`: Initial exploration rate for the attacker.\n",
    "#    - `min_epsilon (0.82)`: Minimum exploration rate for the attacker.\n",
    "#    - `gamma (0.001)`: Discount factor for future rewards.\n",
    "#    - `decay_rate (0.99)`: Epsilon decay rate.\n",
    "#    - `hidden_layers (3)`: Number of hidden layers in the attacker's neural network.\n",
    "#    - `hidden_size (100)`: Number of neurons per hidden layer.\n",
    "#    - `learning_rate (0.2)`: Learning rate for the attacker.\n",
    "#    - `mem_size (1000)`: Replay memory size.\n",
    "# -------------------------------------------------------------------- #\n",
    "\n",
    "# Load your original dataset\n",
    "formated_test_path = \"formated_test_adv_\" + ismModifDataset + \".data\"\n",
    "env_test = RLenv('test', formated_test_path=formated_test_path)\n",
    "\n",
    "\n",
    "formated_train_path = \"formated_train_adv_\" + ismModifDataset + \".data\"\n",
    "formated_test_path = \"formated_test_adv_\" + ismModifDataset + \".data\"\n",
    "\n",
    "# Train batch\n",
    "batch_size = 1\n",
    "minibatch_size = 100\n",
    "ExpRep = True\n",
    "iterations_episode = 100\n",
    "\n",
    "# Initialization of the environment\n",
    "env = RLenv(\"train\",batch_size=batch_size,\n",
    "            iterations_episode=iterations_episode)  \n",
    "obs_size = env.data_shape[1] - len(env.all_attack_names)\n",
    "\n",
    "# Load original training data\n",
    "states, labels = env.get_full()\n",
    "#print('States ',states.columns,states.shape)\n",
    "#print('LABELS ',labels.columns,labels.shape)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {name: idx for idx, name in enumerate(env.attack_names)}\n",
    "numerical_labels = np.array([label_mapping[label.idxmax()] for _, label in labels.iterrows()])\n",
    "\n",
    "print(env.attack_names)\n",
    "# Initialize the GAN model with appropriate parameters\n",
    "print(states.shape[1])\n",
    "noise_dim = 500  # Change to match generator input\n",
    "# Initialize GAN with arguments: batch_size, lr, noise_dim, data_dim, layers_dim, num_classes\n",
    "gan_args = [128, 1e-4, noise_dim, states.shape[1], 128, len(env.attack_names)]\n",
    "gan = GAN(gan_args,env.attack_names)\n",
    "\n",
    "# Prepare the data for GAN\n",
    "data = env.df.copy(deep=True)\n",
    "\n",
    "# Identify the columns to remove (those that are in env.all_attack_names but not in env.attack_names)\n",
    "columns_to_remove = list(set(env.all_attack_names) - set(env.attack_names))\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "# Now `data` contains only the columns from `env.attack_names`\n",
    "\n",
    "# Train the GAN model\n",
    "# Initialize training parameters: cache_prefix, epochs, sample_interval, num_classes\n",
    "train_args = ['gan_model', 0, 200, len(env.attack_names)]\n",
    "print(data.shape)\n",
    "gan.train(data, train_args)\n",
    "\n",
    "# Generate synthetic samples for underrepresented classes\n",
    "samples_per_class = 0\n",
    "synthetic_states = []\n",
    "synthetic_labels = []\n",
    "\n",
    "# Print the number of each label before adding synthetic data\n",
    "print(\"Label counts before adding synthetic data:\")\n",
    "prev_label_counts = data[env.attack_names].sum()\n",
    "print(prev_label_counts)  # Print the counts of each label before adding synthetic data\n",
    "\n",
    "# Append the counts to the log, converting the Series to a string\n",
    "append_log(\"Label counts before adding synthetic data:\\n\" + str(prev_label_counts))\n",
    "# Generate and add synthetic data\n",
    "for class_idx, attack_name in enumerate(env.attack_names):\n",
    "    class_count = np.sum(labels[attack_name] == 1)  # Count samples for the current attack\n",
    "    if class_count < samples_per_class:\n",
    "        samples_needed = samples_per_class - class_count\n",
    "\n",
    "        # Create synthetic labels separately\n",
    "        synthetic_labels_batch = np.zeros((samples_needed, len(env.attack_names)))  \n",
    "        synthetic_labels_batch[:, class_idx] = 1  # Set only the current attack label to 1\n",
    "\n",
    "        noise = np.random.normal(0, 1, (samples_needed, noise_dim))  # Generate noise\n",
    "        synthetic_class_samples = gan.generator.predict([noise, synthetic_labels_batch])  # Generate synthetic data\n",
    "\n",
    "        synthetic_states.append(synthetic_class_samples)\n",
    "        synthetic_labels.append(synthetic_labels_batch)  # Use synthetic_labels_batch instead of labels\n",
    "\n",
    "# Combine synthetic data\n",
    "if synthetic_states:\n",
    "    synthetic_states = np.vstack(synthetic_states)\n",
    "    synthetic_labels = np.vstack(synthetic_labels)\n",
    "\n",
    "    # Create DataFrame for synthetic data\n",
    "    synthetic_df = pd.DataFrame(synthetic_states, columns=states.columns)\n",
    "    synthetic_labels_df = pd.DataFrame(synthetic_labels, columns=env.attack_names)\n",
    "\n",
    "    # Combine synthetic states and labels\n",
    "    synthetic_df = pd.concat([synthetic_df, synthetic_labels_df], axis=1)\n",
    "\n",
    "    # Append synthetic data to the original DataFrame\n",
    "    env.df = pd.concat([env.df, synthetic_df], ignore_index=True)\n",
    "\n",
    "    # Shuffle the updated DataFrame\n",
    "    env.df = shuffle(env.df, random_state=random_seed)\n",
    "\n",
    "\n",
    "# Print the number of each label after adding synthetic data\n",
    "print(\"Label counts after adding synthetic data:\")\n",
    "print(env.df[env.attack_names].sum())  # Sum across the label columns to count occurrences of each attack class\n",
    "append_log(\"Label counts after adding synthetic data:\\n\" + str(env.df[env.attack_names].sum()))\n",
    "\n",
    "# Calculate and print the number of labels added\n",
    "added_labels = env.df[env.attack_names].sum() - prev_label_counts\n",
    "print(\"Number of labels added (per attack class):\")\n",
    "print(added_labels)\n",
    "append_log(\"Number of labels added (per attack class):\\n\" + str(added_labels))\n",
    "\n",
    "# Print the total number of samples for each label (before and after)\n",
    "print(\"\\nTotal number of samples for each label (before and after):\")\n",
    "append_log(\"\\nTotal number of samples for each label (before and after):\\n\" + str(prev_label_counts) + \"\\n\" + str(env.df[env.attack_names].sum()))\n",
    "\n",
    "\n",
    "\n",
    "# Update states and labels attributes to reflect the new DataFrame\n",
    "env.states = env.df.drop(columns=env.attack_names)\n",
    "env.labels = env.df[env.attack_names]\n",
    "\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "defender_valid_actions = list(range(len(env.attack_types)))\n",
    "defender_num_actions = len(defender_valid_actions)\n",
    "\n",
    "def_epsilon = 1\n",
    "def_min_epsilon = 0.01\n",
    "def_gamma = 0.001\n",
    "def_decay_rate = 0.999\n",
    "\n",
    "def_hidden_size = 100\n",
    "def_hidden_layers = 2\n",
    "def_learning_rate = 0.00025\n",
    "\n",
    "defender_agent = DefenderAgent(defender_valid_actions, obs_size, \"EpsilonGreedy\",\n",
    "                               epoch_length=iterations_episode,\n",
    "                               epsilon=def_epsilon,\n",
    "                               min_epsilon=def_min_epsilon,\n",
    "                               decay_rate=def_decay_rate,\n",
    "                               gamma=def_gamma,\n",
    "                               hidden_size=def_hidden_size,\n",
    "                               hidden_layers=def_hidden_layers,\n",
    "                               minibatch_size=minibatch_size,\n",
    "                               mem_size=1000,\n",
    "                               learning_rate=def_learning_rate,\n",
    "                               ExpRep=ExpRep)\n",
    "\n",
    "attack_valid_actions = list(range(len(env.attack_names)))\n",
    "attack_num_actions = len(attack_valid_actions)\n",
    "\n",
    "att_epsilon = 1\n",
    "att_min_epsilon = 0.99\n",
    "att_gamma = 0.001\n",
    "att_decay_rate = 0.99\n",
    "\n",
    "att_hidden_layers = 1\n",
    "att_hidden_size = 100\n",
    "att_learning_rate = 0.00025\n",
    "\n",
    "attacker_agent = AttackAgent(attack_valid_actions, obs_size, \"EpsilonGreedy\",\n",
    "                             epoch_length=iterations_episode,\n",
    "                             epsilon=att_epsilon,\n",
    "                             min_epsilon=att_min_epsilon,\n",
    "                             decay_rate=att_decay_rate,\n",
    "                             gamma=att_gamma,\n",
    "                             hidden_size=att_hidden_size,\n",
    "                             hidden_layers=att_hidden_layers,\n",
    "                             minibatch_size=minibatch_size,\n",
    "                             mem_size=1000,\n",
    "                             learning_rate=att_learning_rate,\n",
    "                             ExpRep=ExpRep)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Collect all parameters and statistics into a dictionary\n",
    "# ---------------------------\n",
    "modif_desc_dict = {\n",
    "    \"ismModifDataset\": ismModifDataset,\n",
    "    \"formated_train_path\": formated_train_path,\n",
    "    \"formated_test_path\": formated_test_path,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"minibatch_size\": minibatch_size,\n",
    "    \"ExpRep\": ExpRep,\n",
    "    \"iterations_episode\": iterations_episode,\n",
    "    \"obs_size\": obs_size,\n",
    "    \"noise_dim\": noise_dim,\n",
    "    \"gan_args\": gan_args,\n",
    "    \"train_args\": train_args,\n",
    "    \"samples_per_class\": samples_per_class,\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"defender_valid_actions\": defender_valid_actions,\n",
    "    \"defender_num_actions\": defender_num_actions,\n",
    "    \"def_epsilon\": def_epsilon,\n",
    "    \"def_min_epsilon\": def_min_epsilon,\n",
    "    \"def_gamma\": def_gamma,\n",
    "    \"def_decay_rate\": def_decay_rate,\n",
    "    \"def_hidden_size\": def_hidden_size,\n",
    "    \"def_hidden_layers\": def_hidden_layers,\n",
    "    \"def_learning_rate\": def_learning_rate,\n",
    "    \"attack_valid_actions\": attack_valid_actions,\n",
    "    \"attack_num_actions\": attack_num_actions,\n",
    "    \"att_epsilon\": att_epsilon,\n",
    "    \"att_min_epsilon\": att_min_epsilon,\n",
    "    \"att_gamma\": att_gamma,\n",
    "    \"att_decay_rate\": att_decay_rate,\n",
    "    \"att_hidden_layers\": att_hidden_layers,\n",
    "    \"att_hidden_size\": att_hidden_size,\n",
    "    \"att_learning_rate\": att_learning_rate,\n",
    "    # Training summary info\n",
    "    \"Total_epoch\": num_episodes,\n",
    "    \"Iterations_per_epoch\": iterations_episode,\n",
    "    \"Total_Samples\": num_episodes * iterations_episode,\n",
    "    \"Dataset_shape\": env.get_shape(),\n",
    "    \"Attacker_parameters\": {\n",
    "        \"Num_actions\": attack_num_actions,\n",
    "        \"gamma\": att_gamma,\n",
    "        \"epsilon\": att_epsilon,\n",
    "        \"ANN_hidden_size\": att_hidden_size,\n",
    "        \"ANN_hidden_layers\": att_hidden_layers\n",
    "    },\n",
    "    \"Defense_parameters\": {\n",
    "        \"Num_actions\": defender_num_actions,\n",
    "        \"gamma\": def_gamma,\n",
    "        \"epsilon\": def_epsilon,\n",
    "        \"ANN_hidden_size\": def_hidden_size,\n",
    "        \"ANN_hidden_layers\": def_hidden_layers\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "append_log(\"\\n\".join([f\"{key}: {value}\" for key, value in modif_desc_dict.items()]))\n",
    "\n",
    "# Statistics\n",
    "att_reward_chain = []\n",
    "def_reward_chain = []\n",
    "att_loss_chain = []\n",
    "def_loss_chain = []\n",
    "def_total_reward_chain = []\n",
    "att_total_reward_chain = []\n",
    "\n",
    "stats_desc = \"\\n-------------------------------------------------------------------------------\"\n",
    "stats_desc += f\"\\nTotal epoch: {num_episodes} | Iterations in epoch: {iterations_episode} \" \\\n",
    "              f\"| Minibatch from mem size: {minibatch_size} | Total Samples: {num_episodes * iterations_episode}|\"\n",
    "stats_desc += \"\\n-------------------------------------------------------------------------------\"\n",
    "stats_desc += f\"\\nDataset shape: {env.get_shape()}\"\n",
    "stats_desc += \"\\n-------------------------------------------------------------------------------\"\n",
    "stats_desc += f\"\\nAttacker parameters: Num_actions={attack_num_actions} | gamma={att_gamma} | \" \\\n",
    "              f\"epsilon={att_epsilon} | ANN hidden size={att_hidden_size} | \" \\\n",
    "              f\"ANN hidden layers={att_hidden_layers}|\"\n",
    "stats_desc += \"\\n-------------------------------------------------------------------------------\"\n",
    "stats_desc += f\"\\nDefense parameters: Num_actions={defender_num_actions} | gamma={def_gamma} | \" \\\n",
    "              f\"epsilon={def_epsilon} | ANN hidden size={def_hidden_size} | \" \\\n",
    "              f\"ANN hidden layers={def_hidden_layers}|\"\n",
    "stats_desc += \"\\n-------------------------------------------------------------------------------\"\n",
    "\n",
    "print(stats_desc)\n",
    "append_log(stats_desc)\n",
    "\n",
    "\n",
    "# Main loop\n",
    "attacks_by_epoch = []\n",
    "attack_labels_list = []\n",
    "for epoch in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    att_loss = 0.\n",
    "    def_loss = 0.\n",
    "    def_total_reward_by_episode = 0\n",
    "    att_total_reward_by_episode = 0\n",
    "    states = env.reset()\n",
    "    \n",
    "    attack_actions = attacker_agent.act(states)\n",
    "    states = env.get_states(attack_actions)\n",
    "    done = False\n",
    "    \n",
    "    attacks_list = []\n",
    "    for i_iteration in range(iterations_episode):\n",
    "        attacks_list.append(attack_actions[0])\n",
    "        defender_actions = defender_agent.act(states)\n",
    "        next_states, def_reward, att_reward, next_attack_actions, done = env.act(defender_actions, attack_actions)\n",
    "        \n",
    "        attacker_agent.learn(states, attack_actions, next_states, att_reward, done)\n",
    "        defender_agent.learn(states, defender_actions, next_states, def_reward, done)\n",
    "        \n",
    "        if ExpRep and epoch * iterations_episode + i_iteration >= minibatch_size:\n",
    "            def_loss += defender_agent.update_model()\n",
    "            att_loss += attacker_agent.update_model()\n",
    "        elif not ExpRep:\n",
    "            def_loss += defender_agent.update_model()\n",
    "            att_loss += attacker_agent.update_model()\n",
    "        \n",
    "        states = next_states\n",
    "        attack_actions = next_attack_actions\n",
    "        def_total_reward_by_episode += np.sum(def_reward, dtype=np.int32)\n",
    "        att_total_reward_by_episode += np.sum(att_reward, dtype=np.int32)\n",
    "    \n",
    "    attacks_by_epoch.append(attacks_list)\n",
    "    def_reward_chain.append(def_total_reward_by_episode)\n",
    "    att_reward_chain.append(att_total_reward_by_episode)\n",
    "    def_loss_chain.append(def_loss)\n",
    "    att_loss_chain.append(att_loss)\n",
    "    \n",
    "    test_states, test_labels = env_test.get_full()\n",
    "    num_samples = test_labels.shape[0]\n",
    "    num_classes = len(env_test.attack_types)\n",
    "    mapped_labels = np.zeros((num_samples, num_classes), dtype='float32')\n",
    "    \n",
    "    for i, (idx, row) in enumerate(test_labels.iterrows()):\n",
    "        detailed_label = row.idxmax()\n",
    "        main_category = env_test.attack_map[detailed_label]\n",
    "        class_index = env_test.attack_types.index(main_category)\n",
    "        mapped_labels[i, class_index] = 1.0\n",
    "    \n",
    "    average_test_loss = defender_agent.model_network.model.evaluate(test_states, mapped_labels, verbose=0)\n",
    "    test_loss = average_test_loss * 10  #test_states.shape[0]\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    test_loss_chain.append(test_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\r\\n|Epoch {epoch:03d}/{num_episodes:03d}| time: {end_time - start_time:.2f}|\\r\\n\"\n",
    "          f\"|Def Loss {def_loss:.4f} | Def Reward in ep {def_total_reward_by_episode:03d}|\\r\\n\"\n",
    "          f\"|Att Loss {att_loss:.4f} | Att Reward in ep {att_total_reward_by_episode:03d}|\")\n",
    "    print(f\"|Def Estimated: {env.def_estimated_labels}| Att Labels: {env.def_true_labels}\")\n",
    "    attack_labels_list.append(env.def_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ee7fb-cb66-484e-bc5b-26f15f76b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models_AWID_'+ismModifDoss):\n",
    "    os.makedirs('models_AWID_'+ismModifDoss)\n",
    "\n",
    "\n",
    "###MOD-2\n",
    "\n",
    "# Save as a single H5 file\n",
    "# Ensure the directory exists\n",
    "model_dir = \"models_AWID_\" + ismModifDoss\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "h5_model_path = os.path.join(model_dir, \"defender_agent_model_Full_\"+ismModif+\".h5\")\n",
    "defender_agent.model_network.model.save(h5_model_path)\n",
    "\n",
    "\n",
    "# Save the entire model (TensorFlow format, recommended)\n",
    "full_model_path = os.path.join(model_dir, \"defender_agent_model\")\n",
    "\n",
    "print(f\"✅ Full model saved to: {h5_model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('results_AWID_'+ismModifDoss):\n",
    "    os.makedirs('results_AWID_'+ismModifDoss)    \n",
    "\n",
    "model_dir = \"models_AWID_\" + ismModifDoss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(len(def_reward_chain)),def_reward_chain,label='Defense')\n",
    "plt.plot(np.arange(len(att_reward_chain)),att_reward_chain,label='Attack')\n",
    "plt.title('Total reward by episode')\n",
    "plt.xlabel('n Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(len(def_loss_chain)), def_loss_chain, label='Training Loss (Defender)')\n",
    "plt.plot(np.arange(len(att_loss_chain)), att_loss_chain, label='Training Loss (Attacker)')\n",
    "plt.plot(np.arange(len(test_loss_chain)), test_loss_chain, label='Test Loss (Defender)', linestyle='dashed')\n",
    "plt.title('Loss by episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "       ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig('results_AWID_'+ismModifDoss+'/loss_comparison_'+ismModif+'.svg', format='svg', dpi=1000)\n",
    "\n",
    "\n",
    "# Ensure the number of bins matches the number of attack names\n",
    "bins = np.arange(len(env_test.attack_names)) - 0.5  # Ensure correct bin count\n",
    "\n",
    "valid_epochs = [e for e in [0, 70, 90] if e < len(attacks_by_epoch)]\n",
    "\n",
    "plt.figure(2, figsize=[12,5])\n",
    "plt.title(\"Attacks distribution throughout episodes\")\n",
    "\n",
    "for indx, e in enumerate(valid_epochs):\n",
    "    plt.subplot(1, len(valid_epochs), indx + 1)  # Adjust for available data\n",
    "    plt.hist(attacks_by_epoch[e], bins=bins, width=0.9, align='left')\n",
    "    plt.xlabel(f\"Epoch {e}\")\n",
    "    \n",
    "    # Make sure the number of bins matches the number of attack names\n",
    "    plt.xticks(np.arange(len(env_test.attack_names)), env_test.attack_names, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'results_AWID_{ismModifDoss}/Attacks_distribution_{ismModif}.svg', format='svg', dpi=1000)\n",
    "# MOD-1 Stop\n",
    "\n",
    " # Plot attacks distribution alongside -MOD_1\n",
    "\n",
    "valid_epochs = [e for e in [0,10,20,30,40,60,70,80,90] if e < len(attack_labels_list)]\n",
    "\n",
    "plt.figure(3, figsize=[10,10])\n",
    "plt.title(\"Attacks (mapped) distribution throughout episodes\")\n",
    "\n",
    "for indx, e in enumerate(valid_epochs):\n",
    "    plt.subplot(3, 3, indx + 1)  # Adjust for available data\n",
    "    plt.bar(range(4), attack_labels_list[e], tick_label=['normal', 'flooding', 'injection', 'impersonation'])\n",
    "    plt.xlabel(f\"Epoch {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results_AWID_{ismModifDoss}/Attacks_mapped_distribution_{ismModif}.svg', format='svg', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255970c-25bb-4992-a8a3-e4c755d0aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "formated_test_path = \"formated_test_adv_\" + ismModifDataset + \".data\"\n",
    "\n",
    "env_test = RLenv('test', formated_test_path=formated_test_path)\n",
    "\n",
    "\n",
    "def append_log(content):\n",
    "    \"\"\"Appends a string to the log file inside the model directory.\"\"\"\n",
    "    with open(log_file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(content + \"\\n\")\n",
    "\n",
    "# Define paths\n",
    "model_dir = \"models_AWID_\" + ismModifDoss\n",
    "h5_model_path = os.path.join(model_dir, \"defender_agent_model_Full_\" + ismModif + \".h5\")\n",
    "formated_test_path = \"formated_test_adv_\" + ismModifDataset + \".data\"\n",
    "\n",
    "# Load the trained model from H5 file\n",
    "model = load_model(h5_model_path, compile=False)\n",
    "\n",
    "# Compile model (optional: specify loss and optimizer if needed)\n",
    "model.compile(loss=huber_loss, optimizer=\"sgd\")\n",
    "\n",
    "# Define environment\n",
    "env_test = RLenv('test', formated_test_path=formated_test_path)\n",
    "\n",
    "total_reward = 0    \n",
    "\n",
    "true_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "estimated_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "estimated_correct_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "\n",
    "# Load test data\n",
    "states, labels = env_test.get_full()\n",
    "\n",
    "start_time = time.time()\n",
    "q = model.predict(states)\n",
    "actions = np.argmax(q, axis=1)\n",
    "\n",
    "maped = []\n",
    "for indx, label in labels.iterrows():\n",
    "    maped.append(env_test.attack_types.index(env_test.attack_map[label.idxmax()]))\n",
    "\n",
    "labels, counts = np.unique(maped, return_counts=True)\n",
    "true_labels[labels] += counts\n",
    "\n",
    "for indx, a in enumerate(actions):\n",
    "    estimated_labels[a] += 1              \n",
    "    if a == maped[indx]:\n",
    "        total_reward += 1\n",
    "        estimated_correct_labels[a] += 1\n",
    "\n",
    "# One-hot encoding for evaluation metrics\n",
    "action_dummies = pd.get_dummies(actions)\n",
    "posible_actions = np.arange(len(env_test.attack_types))\n",
    "for non_existing_action in posible_actions:\n",
    "    if non_existing_action not in action_dummies.columns:\n",
    "        action_dummies[non_existing_action] = np.uint8(0)\n",
    "labels_dummies = pd.get_dummies(maped)\n",
    "\n",
    "# Compute F1 Scores for each attack type\n",
    "f1_scores = [\n",
    "    f1_score(labels_dummies[i].values, action_dummies[i].values)\n",
    "    for i in range(len(env_test.attack_types))\n",
    "]\n",
    "\n",
    "# Compute Accuracy\n",
    "Mismatch = estimated_labels - true_labels\n",
    "acc = float(100 * total_reward / len(states))\n",
    "\n",
    "# Print results\n",
    "print('\\nTotal reward: {} | Number of samples: {} | Accuracy = {:.2f}%'.format(\n",
    "      total_reward, len(states), acc))\n",
    "append_log('\\nTotal reward: {} | Number of samples: {} | Accuracy = {:.2f}%'.format(\n",
    "      total_reward, len(states), acc))\n",
    "# Create DataFrame for results\n",
    "outputs_df = pd.DataFrame(index=env_test.attack_types, columns=[\"Estimated\", \"Correct\", \"Total\", \"F1_score\", \"Mismatch\"])\n",
    "for indx, att in enumerate(env_test.attack_types):\n",
    "   outputs_df.iloc[indx].Estimated = estimated_labels[indx]\n",
    "   outputs_df.iloc[indx].Correct = estimated_correct_labels[indx]\n",
    "   outputs_df.iloc[indx].Total = true_labels[indx]\n",
    "   outputs_df.iloc[indx].F1_score = f1_scores[indx] * 100\n",
    "   outputs_df.iloc[indx].Mismatch = abs(Mismatch[indx])\n",
    "\n",
    "print(outputs_df)\n",
    "append_log(outputs_df.to_string())\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "pos = np.arange(len(true_labels))\n",
    "\n",
    "plt.bar(pos, estimated_correct_labels, width, color='g', label=\"Correct estimated\")\n",
    "plt.bar(pos + width, np.abs(estimated_correct_labels - true_labels), width, color='r', label=\"False negative\")\n",
    "plt.bar(pos + width, np.abs(estimated_labels - estimated_correct_labels), width, \n",
    "        bottom=np.abs(estimated_correct_labels - true_labels), color='b', label=\"False positive\")\n",
    "\n",
    "ax.set_xticks(pos + width / 2)\n",
    "ax.set_xticklabels(env_test.attack_types, rotation='vertical', fontsize='xx-large')\n",
    "ax.yaxis.set_tick_params(labelsize=15)\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results_AWID_{ismModifDoss}/test_adv_imp_{ismModif}.svg', format='svg', dpi=1000)\n",
    "\n",
    "# Print evaluation metrics\n",
    "aggregated_data_test = np.array(maped)\n",
    "print('\\nPerformance measures on Test data')\n",
    "append_log('\\nPerformance measures on Test data')\n",
    "print(f'Accuracy = {accuracy_score(aggregated_data_test, actions):.4f}')\n",
    "append_log(f'Accuracy = {accuracy_score(aggregated_data_test, actions):.4f}')\n",
    "print(f'F1 = {f1_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "append_log(f'F1 = {f1_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "print(f'Precision = {precision_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "append_log(f'Precision = {precision_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "print(f'Recall = {recall_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "append_log(f'Recall = {recall_score(aggregated_data_test, actions, average=\"weighted\"):.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "cnf_matrix = confusion_matrix(aggregated_data_test, actions)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=env_test.attack_types, normalize=True, \n",
    "                      title='Normalized confusion matrix')\n",
    "plt.savefig(f'results_AWID_{ismModifDoss}/confusion_matrix_adversarial_{ismModif}.svg', format='svg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a7f0c-de3f-4574-9ff6-94a695108d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in env.df:\", env.df.columns.tolist())\n",
    "print(states.columns)\n",
    "print(env.attack_types)\n",
    "print(f\"Dataset shape: {env.get_shape()}\" , env.df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88cbd2-04ca-4f21-8e12-82ffc64c3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Define paths\n",
    "model_dir = \"models_AWID_\" + ismModifDoss\n",
    "h5_model_path = os.path.join(model_dir, \"defender_agent_model_Full_\" + ismModif + \".h5\")\n",
    "formated_test_path = \"formated_test_adv_\" + ismModifDataset + \".data\"\n",
    "formated_train_path = \"formated_train_adv_\" + ismModifDataset + \".data\"\n",
    "\n",
    "# Load the trained model from H5 file\n",
    "model = load_model(h5_model_path, compile=False)\n",
    "model.compile(loss=huber_loss, optimizer=\"sgd\")\n",
    "\n",
    "# Define environments\n",
    "env_test = RLenv('test', formated_test_path=formated_test_path)\n",
    "env_train = RLenv('train', formated_train_path=formated_train_path)\n",
    "\n",
    "# Load test and train data\n",
    "test_states, test_labels = env_test.get_full()\n",
    "train_states, train_labels = env_train.get_full()\n",
    "\n",
    "# Identify common and exclusive detailed labels\n",
    "train_attack_names = set(env_train.attack_names)\n",
    "test_attack_names = set(env_test.attack_names)\n",
    "common_labels = train_attack_names.intersection(test_attack_names)\n",
    "exclusive_labels = test_attack_names - train_attack_names\n",
    "\n",
    "# Map to main categories for verification\n",
    "train_main_categories = set([env_train.attack_map[label] for label in train_attack_names])\n",
    "test_main_categories = set([env_test.attack_map[label] for label in test_attack_names])\n",
    "common_main_categories = train_main_categories.intersection(test_main_categories)\n",
    "exclusive_main_categories = test_main_categories - train_main_categories\n",
    "\n",
    "print(f\"Train detailed labels: {train_attack_names}\")\n",
    "print(f\"Test detailed labels: {test_attack_names}\")\n",
    "print(f\"Common detailed labels: {common_labels}\")\n",
    "print(f\"Exclusive detailed labels: {exclusive_labels}\")\n",
    "print(f\"Train main categories: {train_main_categories}\")\n",
    "print(f\"Test main categories: {test_main_categories}\")\n",
    "print(f\"Common main categories: {common_main_categories}\")\n",
    "print(f\"Exclusive main categories: {exclusive_main_categories}\")\n",
    "append_log(f\"Train detailed labels: {train_attack_names}\")\n",
    "append_log(f\"Test detailed labels: {test_attack_names}\")\n",
    "append_log(f\"Common detailed labels: {common_labels}\")\n",
    "append_log(f\"Exclusive detailed labels: {exclusive_labels}\")\n",
    "append_log(f\"Train main categories: {train_main_categories}\")\n",
    "append_log(f\"Test main categories: {test_main_categories}\")\n",
    "append_log(f\"Common main categories: {common_main_categories}\")\n",
    "append_log(f\"Exclusive main categories: {exclusive_main_categories}\")\n",
    "\n",
    "# Map test labels to main categories (attack_types)\n",
    "test_mapped_labels = []\n",
    "test_detailed_labels = []\n",
    "for _, label in test_labels.iterrows():\n",
    "    detailed_label = label.idxmax()\n",
    "    main_category = env_test.attack_map[detailed_label]\n",
    "    test_mapped_labels.append(env_test.attack_types.index(main_category))\n",
    "    test_detailed_labels.append(detailed_label)\n",
    "test_mapped_labels = np.array(test_mapped_labels)\n",
    "test_detailed_labels = np.array(test_detailed_labels)\n",
    "\n",
    "# Predict actions for all test data\n",
    "q = model.predict(test_states)\n",
    "actions = np.argmax(q, axis=1)\n",
    "\n",
    "# Function to evaluate a subset of data\n",
    "def evaluate_subset(states, true_labels, detailed_labels, predicted_actions, label_subset, subset_name, attack_types, use_detailed=False):\n",
    "    # Filter indices based on label subset\n",
    "    if use_detailed:\n",
    "        subset_indices = [i for i, d_label in enumerate(detailed_labels) if d_label in label_subset]\n",
    "    else:\n",
    "        subset_indices = [i for i, m_label in enumerate(true_labels) if env_test.attack_types[m_label] in label_subset]\n",
    "    \n",
    "    if not subset_indices:\n",
    "        print(f\"No samples found for {subset_name}\")\n",
    "        append_log(f\"No samples found for {subset_name}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    subset_states = states.iloc[subset_indices]\n",
    "    subset_true_labels = true_labels[subset_indices]\n",
    "    subset_predicted = predicted_actions[subset_indices]\n",
    "\n",
    "    total_reward = np.sum(subset_true_labels == subset_predicted)\n",
    "    num_samples = len(subset_true_labels)\n",
    "    acc = float(100 * total_reward / num_samples) if num_samples > 0 else 0.0\n",
    "\n",
    "    # Compute metrics\n",
    "    action_dummies = pd.get_dummies(subset_predicted)\n",
    "    labels_dummies = pd.get_dummies(subset_true_labels)\n",
    "    all_indices = range(len(attack_types))\n",
    "    action_dummies = action_dummies.reindex(columns=all_indices, fill_value=0)\n",
    "    labels_dummies = labels_dummies.reindex(columns=all_indices, fill_value=0)\n",
    "\n",
    "    f1_scores = [f1_score(labels_dummies[i].values, action_dummies[i].values) for i in all_indices]\n",
    "    mismatch = np.bincount(subset_predicted, minlength=len(attack_types)) - np.bincount(subset_true_labels, minlength=len(attack_types))\n",
    "\n",
    "    # Results DataFrame\n",
    "    outputs_df = pd.DataFrame(index=attack_types, columns=[\"Estimated\", \"Correct\", \"Total\", \"F1_score\", \"Mismatch\"])\n",
    "    estimated_labels = np.bincount(subset_predicted, minlength=len(attack_types))\n",
    "    true_labels_count = np.bincount(subset_true_labels, minlength=len(attack_types))\n",
    "    correct_labels = np.zeros(len(attack_types), dtype=int)\n",
    "    for i, (true, pred) in enumerate(zip(subset_true_labels, subset_predicted)):\n",
    "        if true == pred:\n",
    "            correct_labels[true] += 1\n",
    "\n",
    "    for i, att in enumerate(attack_types):\n",
    "        outputs_df.iloc[i].Estimated = estimated_labels[i]\n",
    "        outputs_df.iloc[i].Correct = correct_labels[i]\n",
    "        outputs_df.iloc[i].Total = true_labels_count[i]\n",
    "        outputs_df.iloc[i].F1_score = f1_scores[i] * 100\n",
    "        outputs_df.iloc[i].Mismatch = abs(mismatch[i])\n",
    "\n",
    "    print(f'\\n{subset_name} - Total reward: {total_reward} | Number of samples: {num_samples} | Accuracy = {acc:.2f}%')\n",
    "    append_log(f'\\n{subset_name} - Total reward: {total_reward} | Number of samples: {num_samples} | Accuracy = {acc:.2f}%')\n",
    "    print(outputs_df)\n",
    "    append_log(outputs_df.to_string())\n",
    "\n",
    "    # Plot and save bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    width = 0.35\n",
    "    pos = np.arange(len(true_labels_count))\n",
    "    plt.bar(pos, correct_labels, width, color='g', label=\"Correct estimated\")\n",
    "    plt.bar(pos + width, np.abs(correct_labels - true_labels_count), width, color='r', label=\"False negative\")\n",
    "    plt.bar(pos + width, np.abs(estimated_labels - correct_labels), width, \n",
    "            bottom=np.abs(correct_labels - true_labels_count), color='b', label=\"False positive\")\n",
    "    ax.set_xticks(pos + width / 2)\n",
    "    ax.set_xticklabels(attack_types, rotation='vertical', fontsize='xx-large')\n",
    "    ax.yaxis.set_tick_params(labelsize=15)\n",
    "    plt.legend(fontsize='x-large')\n",
    "    plt.title(f\"{subset_name} Performance\")\n",
    "    plt.tight_layout()\n",
    "    bar_chart_path = f'results_AWID_{ismModifDoss}/test_bar_{subset_name.lower().replace(\" \", \"_\")}_{ismModif}.svg'\n",
    "    plt.savefig(bar_chart_path, format='svg', dpi=1000)\n",
    "    plt.close()\n",
    "    print(f\"Bar chart saved to: {bar_chart_path}\")\n",
    "    append_log(f\"Bar chart saved to: {bar_chart_path}\")\n",
    "\n",
    "    # Confusion matrix - Plot and save as image\n",
    "    cnf_matrix = confusion_matrix(subset_true_labels, subset_predicted)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=attack_types, normalize=True, title=f'Normalized Confusion Matrix - {subset_name}')\n",
    "    cm_plot_path = f'results_AWID_{ismModifDoss}/test_confusion_matrix_{subset_name.lower().replace(\" \", \"_\")}_{ismModif}.svg'\n",
    "    plt.savefig(cm_plot_path, format='svg', dpi=1000)\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix plot saved to: {cm_plot_path}\")\n",
    "    append_log(f\"Confusion matrix plot saved to: {cm_plot_path}\")\n",
    "\n",
    "\n",
    "    return acc, f1_score(subset_true_labels, subset_predicted, average=\"weighted\"), precision_score(subset_true_labels, subset_predicted, average=\"weighted\"), recall_score(subset_true_labels, subset_predicted, average=\"weighted\")\n",
    "\n",
    "# Test 1: All Labels (using main categories)\n",
    "acc_all, f1_all, prec_all, rec_all = evaluate_subset(test_states, test_mapped_labels, test_detailed_labels, actions, set(env_test.attack_types), \"All Labels\", env_test.attack_types, use_detailed=False)\n",
    "\n",
    "# Test 2: Common Labels (using detailed labels)\n",
    "acc_common, f1_common, prec_common, rec_common = evaluate_subset(test_states, test_mapped_labels, test_detailed_labels, actions, common_labels, \"Common Labels\", env_test.attack_types, use_detailed=True)\n",
    "\n",
    "# Test 3: Exclusive Labels (using detailed labels)\n",
    "acc_exclusive, f1_exclusive, prec_exclusive, rec_exclusive = evaluate_subset(test_states, test_mapped_labels, test_detailed_labels, actions, exclusive_labels, \"Exclusive Labels\", env_test.attack_types, use_detailed=True)\n",
    "\n",
    "# Print summary of performance metrics\n",
    "print('\\nPerformance measures on Test data - Summary')\n",
    "append_log('\\nPerformance measures on Test data - Summary')\n",
    "for name, acc, f1, prec, rec in [\n",
    "    (\"All Labels\", acc_all, f1_all, prec_all, rec_all),\n",
    "    (\"Common Labels\", acc_common, f1_common, prec_common, rec_common),\n",
    "    (\"Exclusive Labels\", acc_exclusive, f1_exclusive, prec_exclusive, rec_exclusive)\n",
    "]:\n",
    "    if acc is not None:  # Only print if there were samples to evaluate\n",
    "        print(f'{name}:')\n",
    "        print(f'Accuracy = {acc/100:.4f}')\n",
    "        print(f'F1 = {f1:.4f}')\n",
    "        print(f'Precision = {prec:.4f}')\n",
    "        print(f'Recall = {rec:.4f}')\n",
    "        append_log(f'{name}:')\n",
    "        append_log(f'Accuracy = {acc/100:.4f}')\n",
    "        append_log(f'F1 = {f1:.4f}')\n",
    "        append_log(f'Precision = {prec:.4f}')\n",
    "        append_log(f'Recall = {rec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602b472-e50c-4856-b0bf-22116b2dac28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d34401-3a60-45a6-ab3e-67240f3bb6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
